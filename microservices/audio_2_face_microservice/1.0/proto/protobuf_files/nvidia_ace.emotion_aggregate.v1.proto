/*
 * SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto3";

package nvidia_ace.emotion_aggregate.v1;

import "nvidia_ace.emotion_with_timecode.v1.proto";

// WARNING: alpha version of the emotion output of Audio2Face Microservice
// This can be subject to change.
// The primary function of this message is to enable the analysis of blendshape
// data associated with the emotion data.
// See the documentation and the provided sample application for more information
message EmotionAggregate {
    // Input emotions refer to the emotions provided by the user of the Audio2Face
    // Service before any processing is applied.
    repeated nvidia_ace.emotion_with_timecode.v1.EmotionWithTimeCode input_emotions = 1;
    // These emotions are a blend between:
    // 1. emotions generated by the Audio2Emotion model
    // 2. input_emotions
    repeated nvidia_ace.emotion_with_timecode.v1.EmotionWithTimeCode a2e_output = 2;
    // These emotions are the emotions coming from a2e_output smoothed over time
    repeated nvidia_ace.emotion_with_timecode.v1.EmotionWithTimeCode a2f_smoothed_output = 3;
}
//nvidia_ace.emotion_aggregate.v1
//v1.0.0